{
  "agent_id": "coder3",
  "task_id": "task_2",
  "files": [
    {
      "name": "requirements.txt",
      "purpose": "Python dependencies",
      "priority": "high"
    },
    {
      "name": "dataset_downloader.py",
      "purpose": "HuggingFace dataset integration",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CL_2508.08192v1_Efficient_Speculative_Decoding_for_Llama_at_Scale",
    "project_type": "transformer",
    "description": "Enhanced AI project based on cs.CL_2508.08192v1_Efficient-Speculative-Decoding-for-Llama-at-Scale with content analysis. Detected project type: transformer (confidence score: 5 matches).",
    "key_algorithms": [
      "Simple",
      "Smaller",
      "Constant",
      "Language",
      "Feed-Forward",
      "Sampling",
      "Context",
      "Eagle",
      "Draft",
      "Machine"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.CL_2508.08192v1_Efficient-Speculative-Decoding-for-Llama-at-Scale.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nEfficient Speculative Decoding for Llama at Scale:\nChallenges and Solutions\nGenAI and Infra Teams at Meta.\u2661\n\u2661A detailed author list can be found in the Contributions section of this paper.\nSpeculative decoding is a standard method for accelerating the inference speed of large language\nmodels. However, scaling it for production environments poses several engineering challenges, including\nefficiently implementing different operations (e.g., tree attention and multi-round speculative decoding)\non GPU. In this paper, we detail the training and inference optimization techniques that we have\nimplemented to enable EAGLE-based speculative decoding at a production scale for Llama models.\nWith these changes, we achieve a new state-of-the-art inference latency for Llama models. For\nexample, Llama4 Maverick decodes at a speed of about 4 ms per token (with a batch size of one) on 8\nNVIDIA H100 GPUs, which is 10% faster than the previously best known method. Furthermore, for\nEAGLE-based speculative decoding, our optimizations enable us to achieve a speed-up for large batch\nsizes between 1.4\u00d7and 2.0\u00d7at production scale.\nDate:August 12, 2025\nCorrespondence: Sachin Mehta at sacmehta@meta.com\n1 Introduction\nAttention-based transformers of Vaswani et al. (2017) are widely used across different areas of machine learning\nand artificial intelligence, including natural language processing, computer vision, and speech processing.\nSpecifically, transformer-based large language models (LLMs) have been scaled to billions of parameters, and\nare trained on trillions of tokens to produce high-quality models (e.g., Achiam et al., 2023; Grattafiori et al.,\n2024; Meta, 2025; Team et al., 2023). Because of their auto-regressive nature and large size, the decoding\nspeed fo these models\u2019 is very slow, posing significant challenges when deployed in production environments\nwhere they must handle a large volume of requests with varying input and output lengths.\nSeveral methods, including FlashAttention (Dao et al., 2022; Dao, 2024), memory-efficient attention Rabe and\nStaats (2021), fully-sharded data parallel (Baines et al., 2021), and disaggregated inference (Zhong et al., 2024;\nQin et al., 2024), have been proposed to improve the training and inference speed of transformer-based models.\nComplementary to these methods, speculative decoding (Leviathan et al., 2023; Chen et al., 2023) has emerged\nas a promising technique for accelerating the inference speed of LLMs. Briefly, speculative decoding involves\npredicting multiple tokens using a smaller model (aka, draft model) auto-regressively, which are validated\nin a single step using an LLM. This reduces the number of calls to the auto-regressive LLM, improving\ndecoding speed. However, this approach requires significantly more floating point operations (FLOPs) than\nthe non-speculative decoding model because multiple tokens need to be validated by the LLM as opposed to\nsingle token decoding in case of a non-speculative decoding setup. Several speculative decoding methods have\nbeen proposed (e.g., Cai et al., 2024; Miao et al., 2024; Li et al., 2024). However, most of these methods are\nbenchmarked at small scale settings (e.g., batch size of one) to demonstrate the speed-up over non-speculative\ndecoding method, likely because of significant engineering challenges in scaling these methods to production\nenvironments that require handling a large volume of dynamic requests efficiently. For example, when the\nbatch size is increased from 2 to 48, the speed-up (measured using vLLM) of EAGLE-based speculative\ndecoding compared to non-speculative decoding drops from 1.3\u00d7to0.7\u00d7(see Table 5 in v3 of Li et al. (2025)).\nSimilar behaviour was also observed with SGLang (SGLang, 2023) (see Table 5 in (Li et al., 2025)).\nIn this paper, we detail the training (see Section 2) and inference (see Section 3) optimization techniques\nwe have implemented to enable EAGLE-based speculative decoding for Llama models at a production scale,\naddressing the challenges associated with deploying these models in real-world applications. Figure 1 shows\n1arXiv:2508.08192v1  [cs.CL]  11 Aug 2025\n\n--- Page 2 ---\nL3.1-8b L3.3-70b L4-Scout L4-Maverick\nLlama models3.54.04.55.05.56.06.5TTIT (ms)\n1.32x\n1.17x\n1.12x\n1.10xEngine\nOurs (Tree)\nOurs (Chain)\nvLLM (Chain)(a)\n1 2 4 8 16 32\nBatch Size4681012TTIT (ms)\nModel\nL3.3-8B\nL3.1-70B\nL4-Scout\nL4-Maverick (b)\n0h 00m7h 30m15h 00m22h 30m30h 15m37h 45m45h 15m52h 45m60h 30m68h 00m3h 30m11h 00m18h 45m26h 15m33h 45m41h 15m49h 00m56h 30m64h 00m71h 45m\nRequest time6.06.26.46.66.87.07.2TTIT (ms)\n(c)\nFigure 1 Performance evaluation of our EAGLE-based speculative decoding system. (a) TTIT comparison of our\nsystem with vLLM at a batch size of one. (b) TTIT comparison of different LLaMA models within our system at\nvarious batch sizes and a context length of 8k. (c) TTIT for online user requests using LLaMA 3.3 70B over a 3-day\nperiod. In (b), we do not compare with vLLM because of significant gaps in TTIT between our system and vLLM.\nThis is likely because vLLM stacks might not be optimized for large batch sizes, as also observed in other speculative\ndecoding works (e.g., Li et al., 2025).\nthat, with our optimizations, the decoding speed (with a batch size of one) of Llama models with EAGLE on\n8 NVIDIA H100 GPUs improves by about 10-30% as compared to widely used open-source library vLLM.\nMoreover, at large batch sizes, our optimizations for EAGLE-based speculative decoding further enable a\nspeed-up between 1.4\u00d7and 2\u00d7. We note that the proposed optimizations are complementary to open-source\nlibraries (e.g., Kwon et al., 2023; SGLang, 2023) and can be easily integrated to further improve the inference\nof supported speculative decoding methods.\n2 Training optimizations\nWe introduce three changes for EAGLE-based speculative decoding: (a) online distillation (see Section 2.1),\n(b) longer training (see Section 2.2), and (c) multi-layer dense draft model (see Section 2.3). With these\n2\n\n--- Page 3 ---\nEmbedding\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\nLayer 1\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\nLayer 2\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\nLayer N\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\nNormalization\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\nLM Head\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512Embedding\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\nFC\nLayer 1\nLayer 2\nLayer 3\nNormalization\nLM Head\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512\u1f512Weight Tie\nWeight Tiet1, t2,\u00b7\u00b7\u00b7, tn\nlb\n1, lb\n2,\u00b7\u00b7\u00b7, lb\nnt2, t3,\u00b7\u00b7\u00b7, tn\nld\n2, ld\n3,\u00b7\u00b7\u00b7, ld\nnShift\nLCEShift\nLL1Shift\nBaseDraft\nFigure 2 Overview of online distillation that we used to train EAGLE speculative decoding method for Llama-3 and\nLlama-4 models. Here, \u1f512represents frozen layers.\nchanges, we trained draft models for four different Llama models that differs in several aspects, including\narchitecture and model size. The results are then presented in Section 2.4.\n2.1 Online distillation\nAn overview of our implementation of EAGLE speculative decoding is given in Figure 2. Specifically, it uses a\nbase model Band a draft model D, both of which are auto-regressive. The base model consists of Nsequential\ntransformer layers, while the draft model is light-weight with Mlayers, where M <<< N . During training,\nthe base model Btakes ntokens t={t1, t2,\u00b7\u00b7\u00b7, tn}as input. It produces hidden states hb={hb\n1, hb\n2,\u00b7\u00b7\u00b7, hb\nn}\n(i.e., output from the normalization layer before LM head) and logits lb={lb\n1, lb\n2,\u00b7\u00b7\u00b7, lb\nn}(i.e., output from\nthe LM head before softmax). For the draft model, the input tokens are shifted to the right by one, resulting\nin\u02c6t={t2, t3,\u00b7\u00b7\u00b7, tn}. These tokens are fed into the embedding layer. Unlike the base model, the draft model\nincludes a fully-connected layer between embedding and the first transformer layer. The layer takes the token\nembeddings of \u02c6tand hidden states hfrom the base model as inputs, and the resulting output is fed to the\nrest of the draft model to produce the hidden states hd={hd\n2, hd\n3,\u00b7\u00b7\u00b7, hb\nn}and logits ld={ld\n2, lb\n3,\u00b7\u00b7\u00b7, lb\nn}.\nTo train the draft model, we minimize the loss between (a) hidden states and (b) logits of the base Band\nthe draft Dmodels. Specifically, we compute the smooth L1 loss between the right shifted hidden states\nof the base model and the hidden states of the draft model, as LL1=l1_loss (hb\n2:n, hd\n2:n). Similarly, we\ncompute the cross entropy loss between shifted logits of the base model and those of the draft model, as\n3\n\n--- Page 4 ---\n25 30 35 40 45 48\nTraining iterations (in thousands)2.502.552.602.652.702.752.802.852.90Average TPC\nBenchmark\nMT-Bench\nInternal(a)# LayersModel configurations TPC \u2191\niRoPE FFN Type MT-Bench Internal\n2YesMoE 2.75 2.55\nDense 2.79 2.58\nNoMoE 2.75 2.55\nDense 2.80 2.58\n3 No Dense 2.87 2.71\n(b)\nFigure 3 Ablation with Llama4 Scout. (a) Effect of training duration on the Llama4 Scout\u2019s TPC on two benchmarks,\nMT-Bench and internal benchmarks. (b) Effect of different draft model design choices for Llama4 Scout base model.\nHere, TPC is measured with chain-like draft, temperature=0, top-p=0.9, and speculation length of three. iRoPE\ndenotes interleaved RoPE.\nLCE=ce_loss (lb\n2:n, ld\n2:n). The final loss is a weighted sum between LL1andLCE, as\nL=\u03bbCE\u00b7 LCE+\u03bbL1\u00b7 LL1 (1)\nwhere \u03bbCEand\u03bbL1are the coefficients to control the contributions of cross entropy and L1 loss, respectively.\nIn our experiments, we use \u03bbCE= 0.1and\u03bbL1= 1.0.\n2.2 Longer training\nWe train draft models for the following four base models belonging to Llama3 (Grattafiori et al., 2024)\nand Llama4 (Meta, 2025) families: (1) Llama3.1 8B, (2) Llama3.1 70B, (3) Llama4 Scout (total and active\nparameters are about 109B and 17B respectively), and (4) Llama4 Maverick (total and active parameters are\nabout 400B and 17B respectively). The models in Llama3 family uses dense feed-forward network (FFN)\nlayers while Llama4 models uses mixture-of-experts (MoE; (Shazeer et al., 2017)) layers. We use the same\nsupervised fine-tuning (SFT) dataset that was used to train the models for a total of 48k iterations, with 2M\ntokens per iteration. We optimize the draft model using Adam Kingma (2014), with a constant learning rate\nof 0.0002 and a weight decay of 0.1. To evaluate the quality of the draft model, we measured the number of\naccepted tokens per drafting-validation stage (or tokens per call; TPC) on two benchmarks: (a) MT-Bench\n(Zheng et al., 2023), a public benchmark that is widely used for measuring speculative decoding performance,\nand (2) a private internal benchmark, that contains a diverse, multi-lingual and harder samples. We used a\nbatch size of one and chain-like draft with speculation length of three for TPC evaluation. A higher value of\nTPC is desirable.\nFigure 3a shows the effect of longer draft training using Llama4 Scout. We see that longer training helps\nimprove the average tokens accepted per drafting step or tokens accepted per call (TPC) on both benchmarks.\nNote that we observe a similar trend in other models as well.\n2.3 Multi-layer dense draft models\nFor the draft model design, we considered transformer blocks with dense and MoE FFN. We also studied the\nrelationship between the number of transformer blocks and the quality metrics. The results are shown in\nFigure 3b. We make following observations:\n\u2022Dense vs. MoE FFN: Our results indicate that draft models with dense FFN achieve comparable TPC to\nMoE, while utilizing substantially fewer parameters. For example, the draft model for Llama4 Maverick\nwith dense FFN layers has about 10\u00d7fewer total parameters as compared to MoE. Therefore, we used\ndense FFN layers in our draft models.\n\u2022Effect of iRoPE: Llama4 introduced interleaved ROPE (iRoPE). We did not observe any significant\ndifferences in TPC or end-to-end latency with and without iRoPE. Therefore, iRoPE was not used in\nthe training of our final draft models.\n4\n\n--- Page 5 ---\nModel Speculation length TPC \u2191\nLlama3.1-8B w/ EAGLE 3/5/7 2.29/2.44/2.47\nLlama3.1-8B w/ EAGLE3 3/5/7 2.80/3.32/3.57\nLlama3.1-8B w/ EAGLE 3/5/7 2.12/2.24/2.27\nLlama3.3-70B w/ EAGLE3 3/5/7 2.64/3.03/3.20\nLlama3.1-8B w/ EAGLE (Ours) 3 2.78\nLlama3.3-70B w/ EAGLE (Ours) 3 2.94\nLlama4-Scout w/ EAGLE (Ours) 3 2.87\nLlama4-Maverick w/ EAGLE (Ours) 3 2.75\nTable 1TPC results for different Llama models on the MT-Bench benchmark. Here, TPC is measured with chain-like\ndraft, temperature=0, and top-p=0.9.\n\u2022Effectofnumberoflayers: We varied the number of layers from 1 to 3 and observed a notable improvement\nin TPC, from 2.63 to 2.87. However, further increasing the model\u2019s depth beyond 3 layers did not yield\nsignificant gains in TPC. Therefore, we choose a 3-layer dense draft model as our final candidate for\nspeculative decoding.\n2.4 Results\nResults for different Llama3 and Llama4 models on the MT-bench benchmark are shown in Table 1. The\nTPC values vary across models, with Llama3.3 70B achieving the highest value of 2.94 and Llama4-Maverick\nachieving the lowest value of 2.75, likely due to differences in model capacity and architecture. As a reference,\nTable 1 also includes the original results for the 8B and 70B models with both EAGLE and EAGLE3 (Li et al.,\n2025), measured using vLLM (Kwon et al., 2023) at different speculation lengths. With the proposed changes,\nEAGLE achieves similar or better TPC than EAGLE3, highlighting the effectiveness of the introduced training\noptimizations. Interestingly, EAGLE with the proposed changes and a speculation length of three outperforms\nthe EAGLE from Li et al. (2024) even at a speculation length of seven.\n3 Inference optimizations\nOur EAGLE speculative decoding inference process, shown in Figure 4, is organized into the following stages:\n(1)Prefilling: This stage involves executing the base model prefill on input prompt tokens, followed by the\ndraft model prefill on the input tokens and hidden states produced by the base model. (2) Tree Dispatcher: In\nthis stage, an optimal static tree structure is selected for the current batch. (3) Drafting: The EAGLE drafter\ntakes the previous token and its hidden state from the base model, running draft model auto-regressively\nto construct a tree of draft tokens. (4) Validation: Here, the draft tokens are verified using the base model.\nThe tree-attention-based inference enables efficient one-shot validation of the entire tree of draft tokens. (5)\nSampling: This stage involves deciding whether to accept some or all of the proposed tokens. Following\nEAGLE, we use multi-round speculative sampling which preserves the output probability distribution of the\noriginal LLM model. (6) Bookkeeping: EAGLE speculative decoding requires caching hidden states for the\nnext round of drafting. Therefore, during bookkeeping, the KV cache and hidden states are rewounded to the\nappropriate position based on the accepted length.\n3.1 Tree attention\nTree attention (Miao et al., 2024) is an important technique in EAGLE speculative decoding, and is used in\nboth drafting and verification stages. During each decoding round, one path in the tree is partially accepted\nbased on an acceptance criterion, which utilizes validation logits generated by the target model at each node.\nA naive approach to compute these logits would involve unrolling all possible paths and performing a prefill\ncomputation with an effective batch size equal to the product of the batch size and the number of paths.\nHowever, a more efficient method would be to flatten all draft tokens into a single sequence. This approach\n5\n\n--- Page 6 ---\nTree dispatcher\nDrafting Prefill Validation Sampling Book keepingAuto-regressive decoding using draft model\n(a)\nLlama3.3 70B (Dense) Llama4 Maverick (MoE)\n1 2 4 8 16 24 32 64\nBatch Size0510152025303540Time (ms)Stages\nValidation\nDrafting\nBookkeeping\nSampling\n1 2 4 8 16 32 48 64\nBatch Size05101520253035Time (ms)Stages\nValidation\nDrafting\nBookkeeping\nSampling\n(b)\nFigure 4 Inference Workflow . (a) Flow diagram illustrating EAGLE speculative decoding in our production environment.\n(b) Example showing a breakdown of the decoding step latency for each stage of the inference workflow at varying\nbatch sizes, measured for Llama3.3 70B and Llama4 Maverick on an NVIDIA H100 GPU. Note that, in this example,\nthree tokens are generated auto-regressively from the draft model and validated by the base model in a single decoding\nstep. Because the decoding step generates multiple tokens, it should not be confused with TTIT, which measures the\nspeed of decoding a single token.\n  I    don't do  like eat want  not    .   like\nPrefix: promptSuffix: flattened tree[system prompt...] [chat history...]  Do you like green eggs and ham?I   don'tdolikeeatwantnot.likeQueriesKeys/values\nFigure 5 An illustration of optimized tree attention. Unlike the standard and unoptimized tree attention (see Figure 4\nin Miao et al. (2024)), we split the attention operation into prefix and suffix for efficient inference.\nintroduces a challenge when computing attention, as the flattened tree tokens are not in the natural sequential\norder, making it hard to apply standard causal attention mask.\nA custom tree mask can be applied by providing an explicit mask tensor. However, tree attention with an\nexplicit mask is slow due to the large shape of the mask, which scales with the query and context (keys/values)\nlength. Therefore, we split the attention computation into two parts: (a) attention between the query and\ncontext (prefix), and (b) attention within the query itself (suffix), as shown in Figure 5. The former is\ncomputationally expensive but mask-free, whereas the latter requires a tree mask but is relatively small.\nThis two pass approach allows us to effectively prepare and extract query/key/values for speculated tokens\nneeded for suffix calculations without any performance overhead. These two attention computations are\n6\n\n--- Page 7 ---\nthen aggregated using merge_attentions , a simple method which computes full attention based on partial\nattention outputs computed on two disjoint chunks of KV context (Juravsky et al., 2024). We have used this\nmethod to implement efficient tree attention in xFormers (Lefaudeux et al., 2022). It\u2019s worth mentioning\nthat our optimized tree attention code in xFormers can be used seamlessly with other tree-based speculative\ndecoding methods, such as SpecInfer (Miao et al., 2024) and Medusa (Cai et al., 2024), to further improve\ntheir inference efficiency.\n3.2 Multi-round speculative sampling\nMulti-round speculative sampling (MSS) in EAGLE extends the standard speculative sampling method of\nLeviathan et al. (2023); Chen et al. (2023), specifically adapting it for tree-like drafts. Naive implementation\nof MSS can introduce significant overhead in production decoding environments due to the nested loops over\ntree depth and the need to launch numerous small kernels. To address this, we implemented the following\noptimizations:\n\u2022PyTorch-2 compilation: While most operations in MSS are not inherently computationally intensive,\nthe nested loop over tree depth and the children of each node can result in significant CPU overhead,\nparticularly in large-scale production environments. To mitigate this, we compiled the code using\nPyTorch-2, achieving a 1.5\u00d7speedup. A key consideration was handling the dynamic batch dimension\nfor variable incoming traffic. Naively applying torch.compile could lead to recompilation for each batch\nsize, potentially causing latency spikes in production. To address this, we designated the batch dimension\nas dynamic in every input tensor. With this change, recompilations are limited to just two cases: batch\nsize of one and batch sizes greater than one. Moreover, if a service receives warm-up traffic at startup,\nand both scenarios are covered, the compiler cache is populated by torch.compile during the warm-up\nphase. This cache is then reused during real traffic, helping to avoid latency spikes.\n\u2022Parallelisation across tensor parallel ranks: Despite the GPU-efficient implementation and\nPyTorch 2 compilation, sampling becomes a noticeable bottleneck in decoding performance at large\nbatch sizes. One primary reason is the application of the top-p mask or nucleus sampling (Holtzman\net al., 2020), which involves inherently \u201cserial\" operations that are challenging to accelerate on the GPU.\nObserving that (a) sampling is embarrassingly parallel over the batch dimension and (b) all tensor\nparallel (TP) ranks should receive the same sampling result, we parallelized the sampling across TP\nranks by padding the batch size to a multiple of the total number of GPUs (aka, world size). However,\nwith this simple solution, special care must be taken to synchronize random number generators across\nranks. Indeed, during sampling, each rank generates random tensors with sizes proportional to the\nbatch size. If different ranks process different local batch sizes, such as when the last rank handles the\nremainder of the global batch size divided by the world size, the random number generators may diverge,\npotentially causing system hangs in subsequent iterations. To prevent this, we generate a full-sized\nrandom tensor on each rank and have each rank take a different slice of this tensor.\n\u2022Greedy draft decoding: Different methods for sampling draft tokens are known for tree-shaped drafts,\nincluding multinomial (with and without replacement) and greedy sampling (Miao et al., 2024; Jeon\net al., 2024). Specifically, for greedy decoding, we consider a node with kchildren, and place ktokens\nwith the highest probabilities into these child nodes. In our experiments, we observed slightly higher\nTPC from greedy sampling compared to other methods. Moreover, the computationally expensive\ntop-p mask is a no-op for draft probabilities with greedy decoding, and allows us to skip top-p mask\nat the drafting stage (while still applying it to target logits at the validation stage). This reduces the\ncomputational overhead from drafting and further helps in improving inference efficiency.\n3.3 Disaggregated inference with large batch sizes\nIn production environments, we use a disaggregated approach, prefill and decode operations are performed\non separate hosts (see Figure 6). In this setup, the client communicates with the decode host, which\nredirects requests to prefill hosts for the first iteration and handles the remaining iterations itself. Due to\nthis disaggregation, speculative decoding must accommodate large batch sizes and variable traffic, which can\nbe challenging as it complicates efficient computational resource management. Furthermore, the increased\n7\n\n--- Page 8 ---\nDraftingValidatingSamplingLaunchDraftingLaunch ValidatingLaunch SamplingSyncBookkeepingPerf StatsSend ResultsHandle Pre\ufb01llTarget Pre\ufb01llDraft Pre\ufb01llD2HSend Results\nBookkeepingHandle Pre\ufb01llSend Results\u2026LaunchDraftingDraftingRound 0Round 1Pre\ufb01ll HostDecode HostSyncCPUGPU(a)Before latency hiding optimizations\nDraftingSamplingLaunchDraftingLaunch ValidatingLaunch SamplingSyncBookkeeping CPUPerf Stats D2HSend ResultsHandle Pre\ufb01llTarget Pre\ufb01llDraft Pre\ufb01llD2HSend ResultsSampling\nSend Token 0LaunchDraftingLaunch ValidatingDraftingBookkeepingHandle Pre\ufb01llSend ResultsPerf Stats Post-processingPerf Stats Post-processing\nValidatingValidatingRound 0Round -1Round 1Pre\ufb01ll Host\nDecode HostSyncCPUGPUBookkeeping GPU\n(b)After latency hiding optimizations\nFigure 6 Disaggregated decoding cycle restructuring optimized inference efficiency by overlapping CPU and GPU tasks.\nThis reduced GPU idle time and improved overall efficiency, as indicated by the red arrows in (a)and (b).\ndemand for FLOPs for larger batch sizes requires careful allocation of hardware resources to ensure that the\nsystem can handle peak loads without bottlenecks, including out-of-memory errors.\nTo handle large batch-sizes with speculative decoding, we implemented the following optimizations that\nimprove execution efficiency and GPU utilization to 94%:\n\u2022We collected hardware traces to identify unnecessary CPU-GPU synchronization points and removed\nthem. This allows the CPU to run ahead of the GPU, ensuring that the GPU has sufficient tasks in\nits work queue. This helps reduce GPU idle time and improve latency. An example trace is shown in\nFigure 7. At the synchronization point highlighted by the blue box in Figure 7a, the CPU waited for\nGPU execution to finish, resulting in inefficiencies and slowdowns in subsequent operations due to CPU\nkernel launch overhead and GPU idleness, as indicated by the red box in Figure 7a. After removing this\nsynchronization point, as shown in Figure 7b, CPU kernels were launched well ahead of GPU execution.\nThis optimization eliminated GPU idle time and improved TTIT by 0.4ms. On an average, we found\nthat TTIT was improved by 8-12% after removing all possible CPU-GPU synchronization points.\n\u2022We restructured the decoding cycle by overlapping CPU operations with GPU kernel execution to further\nimprove latency. Specifically, we decomposed tasks (e.g., bookkeeping) into distinct GPU and CPU\ncomponents. This allowed us to concurrently execute CPU post-processing tasks while GPU kernels\nare running, effectively hiding CPU operations behind GPU processing and improving overall system\nefficiency. As shown in Figure 6, there are gaps between bookkeeping, prefill handling, and drafting, as\n8\n\n--- Page 9 ---\n(a)Before optimization\n(b)After optimization\nFigure 7 An example showing the effect of removing unnecessary CPU-GPU synchronization point from decoding flow.\nInitially, the CPU waited for GPU execution (highlighted as blue box in (a)), causing slowdowns (highlighted as red\nbox in (a)). After removing synchronization points, CPU kernels launched ahead of GPU execution, reducing idle time\n(highlighted as red box in (b)).\nhighlighted by red arrows in Figure 6a. With restructuring, we were able to reduce the GPU idle time\n(see red arrow in Figure 6b), which helped in improving the TTIT on an average by about 10%.\n\u2022To optimize the time to first token (TTFT), we added a sampling step at the end of the prefill phase\n(indicated by the blue box in Figure 6b). This allows us to immediately consume the outputs (hidden\nstate and next predicted token) of the first token as soon as they are received by the decoder, rather\nthan waiting for the entire decoding cycle to complete. By doing so, we reduce latency and improve\nresponse time by approximately 8-30% on average, depending on the traffic volume.\n\u2022We reordered the processing sequence by moving prefill response handling ahead of bookkeeping, allowing\nus to hide it behind the long-running validation kernels. Additionally, we initiated the subsequent round\nof drafting and validating kernels earlier, effectively masking the latency associated with transmitting\nresults to the client.\n9\n\n--- Page 10 ---\n1 7 13 19 25 31 37\nTree size2.02.22.42.62.8Average TPCD3-BF1D4-BF1N5N6N7N8\nD1-BF3D2-BF2\nD1-BF4N9N10N11N12N13N14 N16\nN15\nD3-BF2\nD2-BF3N17N18\nD4-BF2\nD2-BF4D3-BF3\nType\nFull tree\nPrunned tree\n1 7 13 19 25 31 37\nTree size6.506.757.007.257.507.758.008.258.50Average TTIT\nD3-BF1\nD4-BF1\nN5\nN6\nN7\nN8D1-BF3\nD2-BF2D1-BF4\nN9N10\nN11 N12N13\nN14\nN16N15D3-BF2D2-BF3\nN17N18D4-BF2D2-BF4\nD3-BF3\nType\nFull tree\nPrunned tree(a)Llama3.3 70B (Dense)\n1 7 13 19 25 31 37\nTree size2.02.22.42.62.83.03.23.4Average TPC\nD1-BF3D1-BF4D2-BF2D2-BF3D2-BF4\nD3-BF1D3-BF2D3-BF3\nD4-BF1D4-BF2\nN5N6N7N8N9\nN10N11N12N13N14\nN15N16\nN17N18\nType\nFull tree\nPrunned tree\n1 7 13 19 25 31 37\nTree size4.55.05.56.06.57.0Average TTITD1-BF3\nD1-BF4\nD2-BF2\nD2-BF3D2-BF4\nD3-BF1\nD3-BF2D3-BF3\nD4-BF1D4-BF2\nN5\nN6N7\nN8N9\nN10 N11\nN12N13N14\nN15\nN16N17N18 Type\nFull tree\nPrunned tree\n(b)Llama4 Scout (MoE)\nFigure 8 Effect of different tree structures on TPC and TTIT for two different Llama models. We study two different\ntree structures: (1) a full tree with depth mand branching factor n, and is represented as Dm-BF nin graph, and (2)\nprunned tree with inodes, and is represented as Niin a graph. The optimal tree configurations that are used in\nour production environments for both models are different and highlighted in red(N8 and N12 for Llama3 70B and\nLlama4 Scout respectively). Here, TTIT is measured with a batch size of one on NVIDIA H100 GPUs. Here, TPC is\nmeasured with temperature=0, top-p=0.9, and speculation length of three.\n3.4 Other performance optimizations\nPre-computed static trees. In our production environment, the batch size varies widely, ranging from 1 to\nover 100. This variability presents a challenge, as no single tree structure is optimal across the entire range.\nFigure 8 shows that larger tree structures (which are effective at small batch sizes) yield higher TPC but are\nslow. Furthermore, at larger batch sizes, the computational cost outweighs the benefits of increased TPC.\nTo select the optimal tree configurations, we developed tree dispatcher . Based on a pre-computed static tree\nstructures, it decides which tree configuration to use for the current batch size. For example, for a model\ntypically running at small batch sizes we can decide to use a static tree when batch size is 1 and a chain draft\nof 3 tokens when batch size is larger than 1. This way we account for a trade-off between the computational\ncost and TPC, and ensure optimal performance across all traffic conditions.\nDraft KV cache alignment At the beginning of each drafting stage, the EAGLE method overwrites the draft\nmodel\u2019s KV cache by executing a forward pass using the previous round\u2019s newly accepted tokens and hidden\n10\n\n--- Page 11 ---\nstates. Synchronizing the KV cache at this stage ensures that the draft model incorporates relevant contextual\ninformation from the previous validation round, resulting in more accurate and coherent drafting. However,\nthis approach incurs a non-negligible additional computation cost. We found that using only the last output\ntoken and its corresponding hidden states from the previous round is sufficient to align the KV cache of the\ndraft model with the base model while improving inference efficiency.\nCUDA graphs. During inference, particularly with large batch sizes, the drafting and validation stages are\nthe most computationally demanding due to model execution. We expand the usage of CUDA Graphs in\nMeta\u2019s internal inference engine to optimize drafting and validation. This effectively eliminates kernel launch\noverhead and streamlines GPU operations. Notably, we chose to capture the entire model execution in a\nCUDA graph for maximum performance. This approach contrasts with some open-source inference libraries\n(e.g., vLLM-v1 (Kwon et al., 2023)), which separate attention computation from CUDA graph capture to\noffer greater flexibility.\nAttention kernel Attention used in the decoding process of LLMs is bottlenecked by the need to load keys\nand values from the cache located in GPU high-bandwidth memory (HBM). As a result, the runtime scales\nlinearly with the context length. There has been several efforts to implement attention efficiently on a given\nhardware (e.g., Dao et al., 2023, 2022; Dao, 2024).\nFor faster decoding in production environments, we use hardware-specific attention kernels. In our environment,\nwe have three options: (a) Flash Decoding (or Triton Split-k kernel), (b) Flash Attention v2 and v3, and\n(c) Composable Kernel on AMD (ROCm, 2023). Based on the batch size, tree size, and GPU type, we use\na heuristic-based method to select among these attention options, which is primarily derived from the KV\nsplit strategy in these approaches. For example, the flash decoding kernel outperforms the others in terms of\nlatency when the tree size is \u22644and the batch size is \u226464. Flash attention v3 provides the best latency for\nlarger trees and batch sizes when supported by the hardware.\nPaged KV with tree attention Paged KV partitions the KV cache into blocks and allocate the blocks as\nneeded (Kwon et al., 2023). We used paged KV to optimize memory usage, and maintain separate caches for\nbase and draft models. However, paged KV is not compatible with tree attention. To make it compatible, we\nimplemented two changes: (a) the suffix part in tree attention is relatively small. To minimize the overhead of\nlooking up paged blocks, we only apply paging to the prefix attention, and write back the intermediate keys\nand values instead of retrieving them from the cache, and (b) we add additional tree padding when allocating\nblocks for drafting and validation stages to prevent speculative tokens from exceeding block boundaries.\nPersistent KV Our system leverages persistent KV caching to enhance cross-request performance. As\nrequests are processed, least recently used (LRU) blocks from the paged KV cache are evicted to the persistent\nKV cache in order to accommodate new KVs. Then, during prefill, these cached KVs are reused for matching\nprompt prefixes, thereby reducing time to first token (TTFT).\nWe maintain separate persistent KV caches for base and draft models, ensuring that both can benefit from this\noptimization. Additionally, our system enforces that both models show similar cache hit and block eviction\nbehaviors for any given sequence of tokens. This was done primarily to simplify the implementation and\nimprove maintainability. To achieve this uniform behavior, we made the following adjustments:\n\u2022We prefill both the base and draft model KV caches with the entire sequence of prompt tokens. However,\nfor drafting, EAGLE requires the hidden state of the previous token from the base model, which is\nnot available for the first prompt token. Unlike EAGLE, which skips the first prompt token during\ndraft prefill, we fuse the first token with its own hidden state outputted from the base model prefill.\nEmpirically, we have observed that this approach results in a slight increase in TPC.\n\u2022To maintain synchronization between the draft and base models, we scale the draft model\u2019s KV cache\nsize by a factor ofnumber of draft layers\nnumber of base layersrelative to the base model\u2019s KV cache size. This ensures that both\nthe paged and persistent caches of both models have identical sets of blocks, leading to synchronized\nblock eviction times.\n11\n\n--- Page 12 ---\nTPC\u2191 Drafting Latency (ms) \u2193\nBF16 FP8 INT4 BF16 FP8 INT4\nLlama3.1 8B (Dense) 2.79 2.79 2.76 1.00 0.93 0.96\nLlama3.3 70B (Dense) 2.95 2.95 2.96 1.00 0.89 0.83\nLlama4 Scout (MoE) 2.86 2.86 2.87 1.00 0.89 0.87\nLlama4 Maverick (MoE) 2.81 2.79 2.78 1.00 0.93 0.92\nTable 2Effect of FFN quantization in draft models. Here, TPC is measured with chain-like draft, temperature=0, and\ntop-p=0.9 on MT-Bench. Also, drafting latency is the time to generate three tokens (corresponding to speculation\nlength in our experiments) auto-regressively from the draft model.\nQuantized draft model The \u201closslessness\" aspect of speculative decoding is that the draft model\u2019s quality does\nnot affect the final output; it only impacts the speed. Therefore, we can compress the draft model differently\nthan the base model. Table 2 shows that INT4 feed-forward network quantization provides a good trade-off\nbetween TPC and decoding speed.\nGuided decoding Guided decoding helps generate structured outputs and is crucial in production environments\n(Willard and Louf, 2023). To enable speculative decoding for guided decoding requests, we implemented\ntwo changes: (a) We integrated guided decoding logic across all stages of the speculative decoding workflow,\nincluding drafting, sampling, and bookkeeping. (b) We improved performance by optimizing GD-related\noperations on the GPU. Specifically, we efficiently initialized the GD finite state machine (FSM) states and\naccelerated the key step of masking logits by moving CPU-related operations to the GPU. This avoided\nsynchronization overhead, and we observed a speed-up of 2.6\u00d7when guided decoding with speculative decoding\nwas used compared to the non-speculative decoding baseline. We note that results presented in this paper are\nwithout guided decoding.\niRoPE for Llama4 Llama4introducedinterleavedROPE(iRoPE),avariantoflocalattentionwheresequences\nare split into sub-sequences of fixed length, and queries can only attend to values within the same sub-sequence.\nThis effectively modifies the attention mask, making the blocks in the block-diagonal mask smaller. Recall\nthat tree attention (see Section 3.1) also modifies the attention mask. Therefore, it is essential to combine\nthese two modifications correctly for Llama4 inference.\nThe iRoPE attention mask for decoding was implemented using XFormers\u2019 gappy attention biases. We\nadapted the tree attention to use XFormers\u2019 gappy attention biases in prefix attention and integrated iRoPE\nlogic through the validation path. One corner case that we encountered was when the draft string crosses the\nboundary of two sub-sequences. While it is theoretically possible to implement complex logic for splitting the\ntree mask between two sub-sequences, we opted for a simpler solution: truncating the draft to fit entirely\nwithin one sub-sequence. As such situations are extremely rare, the impact on the acceptance rate is negligible.\n3.5 Benchmarking\nMany previous works assumed that speculative decoding speed-up decreases as batch size increases (Su\net al., 2023; Miao et al., 2024; Liu et al., 2024). This is likely because speculative decoding methods may be\nunder-utilizing GPU computational resources , i.e., increased FLOPs from running drafting and validation\ncomes for free because decoding is bound by the GPU memory bandwidth and not compute. However, at\nlarge batch sizes, decoding becomes compute-bound, resulting in a decrease in speedup.\nAt large context lengths, attention dominates the computation even for large batch sizes, and therefore the\nworkload stays memory-bound (Sadhukhan et al., 2025). Our results in Figure 9 partially confirm these\nobservations. Overall, we observe that the relationship between end-to-end speculative decoding speed-up and\nbatch size varies depending on the model size and its performance characteristics. For instance, the Llama3.1\n8B model exhibits greater speculative decoding speedup at large batch sizes compared to small batch sizes. In\ncontrast, the speed-up for Llama4 Maverick, which has approximately 400 billion parameters, decreases with\nincreasing batch size.\n12\n\n--- Page 13 ---\n1 2 4 8 16 32\nBatch Size1.41.51.61.71.81.92.0Speed-up\nSeq. Length\n1k\n2k\n4k\n8k(a)Llama3.1 8B (Dense)\n1 2 4 8 16 32\nBatch Size1.551.601.651.701.751.801.85Speed-up\nSeq. Length\n1k\n2k\n4k\n8k (b)Llama3.1 70B (Dense)\n1 2 4 8 16 32\nBatch Size1.601.651.701.751.801.85Speed-up\nSeq. Length\n1k\n2k\n4k\n8k\n(c)Llama4 Scout (MoE)\n1 2 4 8 16 32\nBatch Size1.31.41.51.61.71.8Speed-up\nSeq. Length\n1k\n2k\n4k\n8k (d)Llama4 Maverick (MoE)\nFigure 9 Inference speed-up of various Llama models using EAGLE speculative decoding, measured relative to the\nbaseline performance. The plot shows how speed-up varies with different batch sizes and sequence lengths.\nContributors\nThis project is the result of efforts by numerous individuals within the GenAI and Infra teams at Meta. Below,\nwe acknowledge all core contributors and contributors, listed in alphabetical order by first name.\nCore contributors. Bangsheng Tang, Carl Chengyan Fu, Fei Kou, Grigory Sizov, Haoci Zhang, Jason Park,\nJiawen Liu, Jie You, Qirui Yang, Sachin Mehta, Shengyong Cai, Xiaodong Wang, Xingyu Liu, Yunlu Li,\nYanjun Zhou, Wei Wei, Zhiwei Zhao, and Zixi Qi.\nContributors. Adolfo Victoria, Aya Ibrahim, Bram Wasti, Changkyu Kim, Daniel Haziza, Fei Sun, Giancarlo\nDelfin, Emily Guo\u20201, Jialin Ouyang, Jaewon Lee, Jianyu Huang, Jeremy Reizenstein, Lu Fang, Quinn Zhu,\nRia Verma\u2020, Vlad Mihailescu, Xingwen Guo, Yan Cui, Ye Hu, and Yejin Lee.\nAcknowledgments\nWe thank Chuanhao Zhuge, Emad El-Haraty, Lai Wei, Mohammad Rastegari\u2020, Rajasi Saha, Seiji Yamamoto,\nSergey Edunov, Shaun Lindsay, Sijia Chen, and Tony Liu for their support. We also thank Edward Yang\nwho helped to make torch.compile work well on multiround speculative sampling. We also thank the broader\nGenAI and Infra team members for the discussions and feedback.\n1\u2020Work done while working at Meta.\n13\n\n--- Page 14 ---\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 ,\n2023.\nMandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth Goyal, Myle Ott, Benjamin Lefaudeux,\nVitaliy Liptchinsky, Mike Rabbat, Sam Sheiffer, et al. Fairscale: A general purpose modular pytorch library for high\nperformance and large scale training, 2021.\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: Simple\nllm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774 , 2024.\nCharlieChen, SebastianBorgeaud, GeoffreyIrving, Jean-BaptisteLespiau, LaurentSifre, andJohnJumper. Accelerating\nlarge language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318 , 2023.\nTri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference\non Learning Representations (ICLR) , 2024.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS) , 2022.\nTri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference. https:\n//crfm.stanford.edu/2023/10/12/flashdecoding.html , October 12 2023. Stanford University.\nAaron Grattafiori et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 , 2024.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In\n8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 .\nOpenReview.net, 2020. https://openreview.net/forum?id=rygGQyrFvH .\nWonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, and Christopher Lott. Recursive speculative\ndecoding: Accelerating llm inference via sampling without replacement. arXiv preprint arXiv:2402.14160 , 2024.\nJordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Y Fu, Christopher R\u00e9, and Azalia Mirhoseini. Hydragen:\nHigh-throughput llm inference with shared prefixes. arXiv preprint arXiv:2402.05099 , 2024.\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.\nWoosuk Kwon et al. Efficient memory management for large language model serving with pagedattention. In Proceedings\nof the ACM SIGOPS 29th Symposium on Operating Systems Principles , 2023.\nBenjamin Lefaudeux et al. xformers: A modular and hackable transformer modelling library. https://github.\ncom/facebookresearch/xformers , 2022. The tree attention implementation can be found at https://github.com/\nfacebookresearch/xformers/blob/8fc8ec5a4d6498ff81c0c418b89bbaf133ae3a44/xformers/ops/tree_attention.py .\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In\nInternational Conference on Machine Learning , pages 19274\u201319286. PMLR, 2023.\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature\nuncertainty. arXiv preprint arXiv:2401.15077 , 2024.\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-3: Scaling up inference acceleration of large language\nmodels via training-time test. arXiv preprint arXiv:2503.01840 , 2025.\nXiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion\nStoica, and Hao Zhang. Optimizing speculative decoding for serving large language models using goodput, 2024.\nhttps://arxiv.org/abs/2406.14066 .\nMeta. LLaMA 4: Multimodal Intelligence. https://ai.meta.com/blog/llama-4-multimodal-intelligence/ , 2025. Ac-\ncessed: 2025-06-05.\nXupeng Miao et al. Specinfer: Accelerating large language model serving with tree-based speculative inference and\nverification. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming\nLanguages and Operating Systems, Volume 3 , pages 932\u2013949, 2024. Figures are referred from this version of the\npaper:https://arxiv.org/abs/2305.09781 .\n14\n\n--- Page 15 ---\nRuoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake: A\nkvcache-centric disaggregated architecture for llm serving. URL https://arxiv. org/abs/2407.00079 , 2024.\nMarkus N Rabe and Charles Staats. Self-attention does not need o(n2)memory. arXiv preprint arXiv:2112.05682 ,\n2021.\nAMD ROCm. Composable kernel. https://github.com/ROCm/composable_kernel , 2023. GitHub repository.\nRanajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian En-Hsu Yen, Avner\nMay, Tianqi Chen, and Beidi Chen. Magicdec: Breaking the latency-throughput tradeoff for long context generation\nwith speculative decoding, 2025. https://arxiv.org/abs/2408.11049 .\nProject SGLang. Sglang: A language for sgl project. https://github.com/sgl-project/sglang , 2023. Accessed:\n2025-06-03.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 ,\n2017.\nQidong Su, Christina Giannoula, and Gennady Pekhimenko. The synergy of speculative decoding and batching in\nserving large language models, 2023. https://arxiv.org/abs/2310.18813 .\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv\npreprint arXiv:2312.11805 , 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.\nBrandon T Willard and R\u00e9mi Louf. Efficient guided generation for llms. arXiv preprint arXiv:2307.09702 , 2023.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\nInformation Processing Systems , 36:46595\u201346623, 2023.\nYinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. {DistServe }:\nDisaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium\non Operating Systems Design and Implementation (OSDI 24) , pages 193\u2013210, 2024.\n15",
  "project_dir": "artifacts/projects/enhanced_cs.CL_2508.08192v1_Efficient_Speculative_Decoding_for_Llama_at_Scale",
  "communication_dir": "artifacts/projects/enhanced_cs.CL_2508.08192v1_Efficient_Speculative_Decoding_for_Llama_at_Scale/.agent_comm",
  "assigned_at": "2025-08-12T21:11:58.151432",
  "status": "assigned"
}